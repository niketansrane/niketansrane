<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Building a machine learning recommendation system to intelligently assign code reviewers based on historical patterns, expertise, and context. A deep dive into ML-powered developer tools.">
    <meta name="author" content="Niketan Rane">
    <meta name="keywords" content="machine learning, code review, reviewer assignment, developer productivity, ML systems, Azure DevOps">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://www.niketansrane.com/blogs/teaching-machines-find-right-reviewers/">
    <meta property="og:title" content="Teaching Machines to Find the Right Reviewers">
    <meta property="og:description" content="Building a machine learning recommendation system to intelligently assign code reviewers based on historical patterns, expertise, and context.">
    <meta property="og:image" content="https://www.niketansrane.com/assets/images/code-reviewer-ml.png">
    <meta property="article:published_time" content="2025-10-01">
    <meta property="article:author" content="Niketan Rane">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Code Review">
    <meta property="article:tag" content="Developer Productivity">
    <meta property="article:tag" content="Azure DevOps">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://www.niketansrane.com/blogs/teaching-machines-find-right-reviewers/">
    <meta property="twitter:title" content="Teaching Machines to Find the Right Reviewers">
    <meta property="twitter:description" content="Building a machine learning recommendation system to intelligently assign code reviewers based on historical patterns, expertise, and context.">
    <meta property="twitter:image" content="https://www.niketansrane.com/assets/images/code-reviewer-ml.png">
    <meta name="twitter:creator" content="@niketansrane">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://www.niketansrane.com/blogs/teaching-machines-find-right-reviewers/">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../../favicon.svg">
    <link rel="alternate icon" href="../../favicon.svg">

    <!-- Theme initialization (prevents flash of wrong theme) -->
    <script>
      (function() {
        try {
          var theme = localStorage.getItem("preferred-theme") || "dark";
          if (theme === 'dark' || theme === 'sublime') {
            document.documentElement.setAttribute('data-theme', theme);
          }
        } catch(e) {}
      })();
    </script>

    <title>Teaching Machines to Find the Right Reviewers - Niketan Rane</title>
    <link rel="stylesheet" href="../../assets/css/base.css">
    <link rel="stylesheet" href="../blog.css">
    <link rel="stylesheet" href="../../assets/css/highlight-github.min.css" data-highlight-theme>
    <script src="../../assets/js/highlight.min.js" defer></script>
    <script defer>document.addEventListener("DOMContentLoaded", function() { hljs.highlightAll(); });</script>

    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "Teaching Machines to Find the Right Reviewers",
        "description": "Building a machine learning recommendation system to intelligently assign code reviewers based on historical patterns, expertise, and context.",
        "image": "https://www.niketansrane.com/assets/images/code-reviewer-ml.png",
        "datePublished": "2025-10-01",
        "author": {
            "@type": "Person",
            "name": "Niketan Rane",
            "url": "https://www.niketansrane.com"
        },
        "publisher": {
            "@type": "Person",
            "name": "Niketan Rane"
        },
        "keywords": ["Machine Learning", "Code Review", "Developer Productivity", "Azure DevOps", "ML Systems"]
    }
    </script>
</head>
<body>
    <div class="blog-wrapper">
        <header class="blog-header">
            <a href="../../" class="blog-home">Niketan Rane</a>
            <nav class="blog-nav">
                <a href="../../">Articles</a>
            </nav>
        </header>
        
        <main class="blog-main">
            <article class="post-content">
                <header class="post-header">
                    <div class="post-meta"><time datetime="2025-10-01" class="post-date">October 2025</time><span class="post-reading-time">9 min read</span></div>
                    <h1 class="post-title">Teaching Machines to Find the Right Reviewers</h1>
                    <div class="post-tags">
                        <span class="post-tag">Machine Learning</span>
                        <span class="post-tag">Code Review</span>
                        <span class="post-tag">Developer Productivity</span>
                        <span class="post-tag">Azure DevOps</span>
                    </div>
                </header>
                
                <div class="prose">
                    <p>When you're dealing with hundreds of pull requests a day, the old way of doing things breaks down pretty fast. Most large codebases which use Azure DevOps for the code management depend on path-based assignment rules to decide reviewers / reviewer groups. It works, but only to an extent. As teams and repositories grow, those rules quickly go stale.</p>

                    <p>More importantly, solving this problem might seem simple on paper by introducing accountability (which quickly goes for a toss unless there is monetary reward associated with an activity like code review) but finding the right reviewers when you have hundreds of pull requests merged in a day is challenging.</p>

                    <p>This post walks through the problem, the ML approach, and what I learned building a data-driven alternative to configuration files.</p>

                    <p><em>Note: I have used AI to help write portions of this post.</em></p>

                    <h2>The Problem</h2>

                    <p>Let me explain this problem with a concrete example. Remember I am talking about Azure DevOps Enterprise (not Github), so some of the terms might be new but any organization that works at scale eventually gets into this same problem.</p>

                    <p>Here's what the enterprise started (rather stuck) with: a path-based system for assigning reviewers. You touch a file in <code>/src/messaging</code>, you get the messaging team as reviewers. Touch something in <code>/components/chat</code>, you get the chat team. Simple solution to start with.</p>

                    <p>Except it doesn't actually work when you scale up. ü•≤</p>

                    <ul>
                        <li>How about a single PR that touched five different directories?</li>
                        <li>Someone who became an expert in a new area six months ago? Too bad, the path rules haven't been updated.</li>
                        <li>That one senior engineer who technically owns <code>/legacy-code</code> but hasn't looked at it in two years? They're getting pinged (spammed) anyway.</li>
                    </ul>

                    <p>And the worst part? The people who <em>should</em> be reviewing certain changes‚Äîthe ones who actually have context, who recently worked on similar code, who could provide valuable feedback‚Äîthey never even see those PRs. Let's call this a discoverability problem.</p>

                    <h2>What If We Just... Asked the Data?</h2>

                    <p>The pros of scale is that you have tremendous data lying around. In this case, the enterprise had years of pull request history just sitting there. Thousands of reviews, hidden patterns that we just had to identify. What if instead of maintaining these rigid path-based rules, I let a machine learning model figure out who the right reviewers actually are?</p>

                    <p>Not based on who owns a directory in a config file, but based on who's actually been doing the work.</p>

                    <p>So I built a machine learning recommendation system that predicts whether a pull request is relevant to a reviewer based on historical patterns.</p>

                    <h2>The Core Approach</h2>

                    <p>Instead of hardcoded rules, I took a classification approach. For any given PR and reviewer pair, the model predicts: is this PR relevant for this person?</p>

                    <p>The model looks at four main dimensions:</p>

                    <p><strong>Sociality:</strong> Do you know the person who wrote this code? If you've reviewed their PRs before, there's probably a reason. Maybe you work closely together, maybe you're both experts in the same domain. That social connection matters. Over time, I also realised that this indirectly correlates with developers reporting to the same leadership without using management chart data.</p>

                    <p><strong>Context:</strong> Do you understand what this PR is trying to do? Not just "have you touched these files," but do you understand the broader problem space? Have you worked on related features recently?</p>

                    <p><strong>Interest alignment:</strong> Does this match your review patterns? Some people gravitate toward performance work, others toward UI, others toward infrastructure. The model picks up on these patterns over time.</p>

                    <p><strong>Expertise:</strong> Are you actually knowledgeable here? This is where we look at your contributions, your review history, your engagement with similar code. Not just whether you own a directory, but whether you really know this stuff.</p>

                    <p>Here's how the entire recommendation pipeline works:</p>

                    <pre class="mermaid">
---
config:
  theme: neutral
  layout: dagre
  look: classic
---
flowchart TB
 subgraph Input["üì• Input"]
        PR["New Pull Request"]
        RC["Reviewer Candidates"]
  end
 subgraph Features["üî¨ Feature Extraction"]
    direction TB
        FE["Feature Engineering"]
        S["Sociality&lt;br&gt;Author-Reviewer History"]
        C["Context&lt;br&gt;File &amp; Problem Space Overlap"]
        I["Interest&lt;br&gt;Review Pattern Matching"]
        E["Expertise&lt;br&gt;Contribution History"]
  end
 subgraph Temporal["‚è±Ô∏è Temporal Windows"]
        TW["Rolling Windows&lt;br&gt;30d | 90d | 180d"]
  end
 subgraph Model["ü§ñ ML Classification"]
        M["Binary Classifier"]
        P{"Is PR relevant&lt;br&gt;for this reviewer?"}
  end
 subgraph Output["üì§ Output"]
        RR["Recommended Reviewers"]
        NR["Not Recommended"]
        RANK["Ranked by&lt;br&gt;Relevance Score"]
  end
    PR --> FE
    RC --> FE
    FE --> S & C & I & E
    S --> TW
    C --> TW
    I --> TW
    E --> TW
    TW --> M
    M -- "For each PR-Reviewer pair" --> P
    P -- Yes --> RR
    P -- No --> NR
    RR --> RANK

    style Input fill:#e1f5fe
    style Features fill:#fff3e0
    style Temporal fill:#f3e5f5
    style Model fill:#e8f5e9
    style Output fill:#fce4ec
                    </pre>

                    <h2>Handling the Time Problem</h2>

                    <p>The clever bit‚Äîand this took some iteration‚Äîis how I handled time. Instead of looking at the complete git history, I used a rolling windows approach: 30 days, 90 days, 180 days.</p>

                    <p>This way, if your focus shifts, the model shifts with you. If you stop working on auth code and move to API design, you'll stop getting auth PRs and start seeing API ones. This temporal dimension helps prevent stale recommendations and ensures the model stays current with people's evolving expertise.</p>

                    <p>We also retrain the model regularly, using only recent historical data. This keeps it from getting stuck in outdated patterns.</p>

                    <h2>Building Features That Actually Matter</h2>

                    <p>The feature engineering was the heart of this. For each PR-reviewer pair, we extract signals across those four dimensions:</p>

                    <ul>
                        <li>How many times has this reviewer worked with this author?</li>
                        <li>What's the overlap between files the reviewer has touched and files in this PR?</li>
                        <li>How recently did the reviewer work on similar changes?</li>
                        <li>What's the reviewer's contribution pattern in related areas?</li>
                    </ul>

                    <p>Each signal gets computed across multiple time windows. So instead of just "has this person reviewed 10 PRs from this author," I know "they reviewed 2 in the last month, 5 in the last quarter, 8 in the last six months." That temporal granularity turns out to be really important.</p>

                    <h2>Avoiding the Reinforcement Trap</h2>

                    <p>Building this wasn't just about getting the model to perform well. It was about avoiding the traps that other systems fell into.</p>

                    <p>There's a natural self-reinforcing bias in reviewer recommendation systems. If you review one type of PR once, you might keep getting assigned similar PRs, which means you'd review them, which means you'd get assigned more, and suddenly you're pigeonholed.</p>

                    <p>I tried to break that cycle by looking beyond just file-level interactions. The sociality introduced diversity based on who you work with, not just what files you touch. The temporal windows let your expertise profile evolve naturally. The fact that we retrain frequently helps ensure new patterns can emerge without being drowned out by old history (say new teams forming for dedicated areas).</p>

                    <p>It's not perfect but it's better than pretending we can hardcode expertise in a configuration file.</p>

                    <h2>What I Learned Along the Way</h2>

                    <p>The biggest lesson? <strong>Feature engineering matters more than model complexity.</strong></p>

                    <p>I didn't use anything exotic‚Äîjust a solid classification model. The magic was in figuring out which signals actually predicted review relevance. File overlap alone wasn't enough. Recency alone wasn't enough. But the combination of sociality, context, interest, and expertise across multiple time windows? That worked.</p>

                    <p>Second lesson: <strong>Temporal features are crucial for systems that evolve.</strong> Codebases change. People's roles change. A static model trained on all-time data gets stale fast. Building time into the feature space itself keeps the recommendations fresh.</p>

                    <p>Third: <strong>Test on realistic data.</strong> I validated this on a large corpus of historical pull requests, looking at whether the model correctly predicted actual review relationships. That ground truth testing caught issues early that would have been painful to discover in production.</p>

                    <h2>What This Approach Enables</h2>

                    <p>Beyond just faster code reviews, this pattern opens up interesting possibilities:</p>

                    <ul>
                        <li><strong>Reduced review fatigue:</strong> People only see PRs they're actually qualified to review</li>
                        <li><strong>Better knowledge distribution:</strong> Junior engineers get matched with senior reviewers who know the relevant domain</li>
                        <li><strong>Discovery of hidden expertise:</strong> The model can surface people who have relevant knowledge but weren't in the hardcoded reviewer lists</li>
                        <li><strong>Adaptive expertise tracking:</strong> As people's skills and focus areas evolve, the recommendations evolve with them</li>
                    </ul>

                    <p>It's a shift from "who should review this based on organizational structure" to "who can actually provide valuable feedback based on demonstrated expertise."</p>

                    <h2>Where This Approach Falls Short</h2>

                    <p>Let me be clear about the limitations:</p>

                    <p><strong>Cold start problems.</strong> New engineers have no review history. The model can't recommend them or suggest what they should review.</p>

                    <p><strong>New code areas.</strong> If nobody's touched a module before, there's no expertise signal to learn from.</p>

                    <p><strong>Bias reinforcement.</strong> The model learns from existing patterns. If your review distribution was inequitable before, it'll stay that way.</p>

                    <p><strong>Rare expertise gets missed.</strong> That one person who knows the legacy billing system but hasn't touched it in six months? The model might not surface them when you need them.</p>

                    <p><strong>Small teams don't need this.</strong> With 10 engineers who all know the codebase, simple round-robin works fine. This approach only makes sense at scale.</p>

                    <h2>The Bigger Picture</h2>

                    <p>Code review is just one place where we rely on rigid rules that don't reflect reality. The same pattern could apply to:</p>

                    <ul>
                        <li><strong>Incident response</strong> (who should be paged for this type of issue?)</li>
                        <li><strong>Design reviews</strong> (who has relevant experience with this architecture?)</li>
                        <li><strong>Onboarding</strong> (who should a new engineer shadow based on their interests?)</li>
                    </ul>

                    <p>Any time you're matching people to work based on expertise, this kind of data-driven approach beats hardcoded rules.</p>

                    <hr>

                    <p>This approach to reviewer recommendation is something I built to solve a real scaling problem in large engineering organizations. If you're working on similar challenges or want to discuss the technical details, I'm always happy to chat about what worked, what didn't, and what I'd do differently next time.</p>
                </div>
                
                <footer class="post-footer">
                    <a href="../../" class="back-link">‚Üê Articles</a>
                </footer>
            </article>
        </main>
        
        <footer class="blog-footer">
            <p>&copy; 2026 Niketan Rane</p>
        </footer>
    </div>
    <script src="../../assets/js/theme.js"></script>
    <script src="../../assets/js/analytics.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js" crossorigin="anonymous"></script>
    <script>mermaid.initialize({ startOnLoad: true, theme: 'neutral' });</script>
</body>
</html>
